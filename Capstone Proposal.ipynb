{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Udacity Machine Learning Engineering Capstone Project Proposal\n",
    "---------------------\n",
    "\n",
    "Author: Giuseppe Romagnuolo\n",
    "\n",
    "<style>\n",
    "    .hide {\n",
    "        font-style: italic;\n",
    "        color:#ccc;\n",
    "    }\n",
    "</style>\n",
    "\n",
    "Riot Games use Data Bricks and ML to identify hate speech during games \n",
    "https://arstechnica.com/gaming/2013/05/using-science-to-reform-toxic-player-behavior-in-league-of-legends/2/\n",
    "\n",
    "https://developers.google.com/machine-learning/crash-course/\n",
    "\n",
    "## Domain Background\n",
    "\n",
    "<p class=\"hide\">\n",
    "Student briefly details background information of the domain from which the project is proposed. Historical information relevant to the project should be included. It should be clear how or why a problem in the domain can or should be solved. Related academic research should be appropriately cited. A discussion of the student's personal motivation for investigating a particular problem in the domain is encouraged but not required.\n",
    "</p>\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "<p class=\"hide\">\n",
    "Student clearly describes the problem that is to be solved. The problem is well defined and has at least one relevant potential solution. Additionally, the problem is quantifiable, measurable, and replicable.\n",
    "</p>\n",
    "\n",
    "## Datasets and Inputs\n",
    "\n",
    "<p class=\"hide\">\n",
    "The dataset(s) and/or input(s) to be used in the project are thoroughly described. Information such as how the dataset or input is (was) obtained, and the characteristics of the dataset or input, should be included. It should be clear how the dataset(s) or input(s) will be used in the project and whether their use is appropriate given the context of the problem.\n",
    "</p>\n",
    "\n",
    "## Solution Statement\n",
    "\n",
    "<p class=\"hide\">\n",
    "Student clearly describes a solution to the problem. The solution is applicable to the project domain and appropriate for the dataset(s) or input(s) given. Additionally, the solution is quantifiable, measurable, and replicable.\n",
    "</p>\n",
    "\n",
    "## Benchmark Model\n",
    "\n",
    "<p class=\"hide\">\n",
    "A benchmark model is provided that relates to the domain, problem statement, and intended solution. Ideally, the student's benchmark model provides context for existing methods or known information in the domain and problem given, which can then be objectively compared to the student's solution. The benchmark model is clearly defined and measurable.\n",
    "</p>\n",
    "\n",
    "## Evaluation Metrics\n",
    "\n",
    "<p class=\"hide\">\n",
    "Student proposes at least one evaluation metric that can be used to quantify the performance of both the benchmark model and the solution model presented. The evaluation metric(s) proposed are appropriate given the context of the data, the problem statement, and the intended solution.\n",
    "</p>\n",
    "\n",
    "This competition uses a newly developed metric that combines several submetrics to balance overall performance with various aspects of unintended bias.\n",
    "\n",
    "Here are defined the submetrics:\n",
    "\n",
    "#### Overall AUC\n",
    "This is the ROC-AUC for the full evaluation set.\n",
    "\n",
    "#### Overall AUC\n",
    "\n",
    "This is the ROC-AUC for the full evaluation set.\n",
    "\n",
    "### Bias AUCs\n",
    "\n",
    "To measure unintended bias, we again calculate the ROC-AUC, this time on three specific subsets of the test set for each identity, each capturing a different aspect of unintended bias. You can learn more about these metrics in Conversation AI's recent paper *[Nuanced Metrics for Measuring Unintended Bias with Real Data in Text Classification](https://arxiv.org/abs/1903.04561)*.\n",
    "\n",
    "**Subgroup AUC:** Here, we restrict the data set to only the examples that mention the specific identity subgroup. *A low value in this metric means the model does a poor job of distinguishing between toxic and non-toxic comments that mention the identity*.\n",
    "\n",
    "**BPSN (Background Positive, Subgroup Negative) AUC:** Here, we restrict the test set to the non-toxic examples that mention the identity and the toxic examples that do not. *A low value in this metric means that the model confuses non-toxic examples that mention the identity with toxic examples that do not*, likely meaning that the model predicts higher toxicity scores than it should for non-toxic examples mentioning the identity.\n",
    "\n",
    "**BNSP (Background Negative, Subgroup Positive) AUC:** Here, we restrict the test set to the toxic examples that mention the identity and the non-toxic examples that do not. *A low value here means that the model confuses toxic examples that mention the identity with non-toxic examples that do not*, likely meaning that the model predicts lower toxicity scores than it should for toxic examples mentioning the identity.\n",
    "\n",
    "### Generalized Mean of Bias AUCs\n",
    "\n",
    "To combine the per-identity Bias AUCs into one overall measure, we calculate their generalized mean as defined below:\n",
    "\n",
    "\n",
    "$M_p(m_s)=( \\frac{1}{N} \\sum_{s=1}^{N}m_{s}^{p} )^{\\frac{1}{p}}$\n",
    "\n",
    "where:\n",
    "\n",
    "MpMp = the ppth power-mean function\\\n",
    "msms = the bias metric mm calulated for subgroup ss\\\n",
    "NN = number of identity subgroups\n",
    "\n",
    "For this competition, we use a pp value of -5 to encourage competitors to improve the model for the identity subgroups with the lowest model performance.\n",
    "\n",
    "### Final Metric\n",
    "\n",
    "We combine the overall AUC with the generalized mean of the Bias AUCs to calculate the final model score:\n",
    "\n",
    "$score=w_0AUC_{overall}+\\sum_{a=1}^{A}w_aM_p(m_{s,a})$\n",
    "\n",
    "where:\n",
    "\n",
    "- $A$ = number of submetrics (3)\n",
    "- $ms_{s,a}$ = bias metric for identity subgroup ss using submetric $a$\n",
    "$w_a$ = a weighting for the relative importance of each submetric; all four $w$ values set to 0.25\n",
    "\n",
    "While the leaderboard will be determined by this single number, we highly recommend looking at the individual submetric results, [as shown in this kernel](https://www.kaggle.com/dborkan/benchmark-kernel), to guide you as you develop your models.\n",
    "\n",
    "## Project Design\n",
    "\n",
    "<p class=\"hide\">\n",
    "Student summarizes a theoretical workflow for approaching a solution given the problem. Discussion is made as to what strategies may be employed, what analysis of the data might be required, or which algorithms will be considered. The workflow and discussion provided align with the qualities of the project. Small visualizations, pseudocode, or diagrams are encouraged but not required.\n",
    "</p>\n",
    "\n",
    "## Presentation\n",
    "\n",
    "<p class=\"hide\">\n",
    "Proposal follows a well-organized structure and would be readily understood by its intended audience. Each section is written in a clear, concise and specific manner. Few grammatical and spelling mistakes are present. All resources used and referenced are properly cited.\n",
    "</p>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
